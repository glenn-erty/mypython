{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 뉴스 크롤링\n",
    "\n",
    "Last update: May 28. 2017  \n",
    "Kyusik Kim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 깔끔하지도 않고 오직 나만을 위해 만든 크롤러이기 때문에 몇 가지 TIP을 공유하는 차원에서 글을 쓴다. (공유라기 보다는 그냥 내가 까먹지 않으려고... 일을 그만 두면서 코드를 가지고 왔다고 생각했는데, 감쪽같이 사라져 새로 만드는 데 난감했기 때문이다)\n",
    "* 먼저 크롤링(crawling)은 우리가 인터넷 서핑할 때 보는 웹 페이지 내에 있는 데이터를 추출해 내는 행위를 말하며, 이를 수행하는 일련의 시스템(?)을 크롤러(crawler)라 할 수 있겠다. 다양한 방법이 있지만, 나는 Python을 이용한다. \n",
    "* Python을 이용해 크롤링을 할 때 핵심이 되는 라이브러리를 열거하면, BeautifulSoup, requests, urllib.request 등이 있다. 보조적으로, pandas, re, time, random 을 이용한다.  \n",
    "![그림1](https://github.com/iamglenn/mypython/blob/master/pic/pic_crawl_1.png?raw=true)\n",
    "\n",
    "\n",
    "* 네이버 뉴스의 경우 기사 제목인 \"포천시~ 참가\" 부분과 빨간색 네모박스의 \"국제뉴스\" 부분의 url이 다르다. 둘의 차이를 설명하면, 기사 제목을 누르면 국제뉴스와 연결되고, 국제뉴스를 누르면 네이버뉴스 플랫폼에서 제공하는 국제뉴스의 기사에 연결된다는 점이 다르다. 이런 것은 좀 더 편하게 크롤링을 하기 위한 꼼수라고 해야 할까?\n",
    "* 순서는 아래의 내용을 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **import module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import urllib.request\n",
    "import re\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **네이버 뉴스 url 입력하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_basic = \"http://news.naver.com/main/search/search.nhn?query=%BE%C6%B4%C2%C7%FC%B4%D4&st=news.all&q_enc=EUC-KR&r_enc=UTF-8&r_format=xml&rp=none&sm=all.basic&ic=all&so=rel.dsc&rcnews=exist:032:005:086:020:021:081:022:023:025:028:038:469:&rcsection=exist:&stDate=range:20170521:20170528&detail=0&pd=3&r_cluster2_start=11&r_cluster2_display=10&start=11&display=10&startDate=2017-05-21&endDate=2017-05-28&page=1\"\n",
    "url_basic = url_basic[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **총 페이지 수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_pages = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **내보낼 때 파일 이름**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = r'\\아는형님'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_url = []\n",
    "press_name = []\n",
    "press_time = []\n",
    "press_title = []\n",
    "press_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **url 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x000001DF80CDFBA8>\n"
     ]
    }
   ],
   "source": [
    "url = url_basic + str(1)\n",
    "source_code_from_URL = urllib.request.urlopen(url)\n",
    "print(source_code_from_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **BeautifulSoup으로 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source_code_from_URL, \"lxml\", from_encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **div의 info class에 해당하는 내용 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "item = soup.find_all('div', 'info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **div class=\"info\"에서 a태그에 달려 있는 각 네이버뉴스의 url 주소 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=106&oid=081&aid=0002824625\n"
     ]
    }
   ],
   "source": [
    "article = item[0].find('a')['href']\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_url = urllib.request.urlopen(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **네이버뉴스에서 추출한 url에서 내용을 BeautifulSoup으로 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(news_url, \"lxml\", from_encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **네이버뉴스에서 div 태그에 있는 articeBody를 가져오기**\n",
    "* **articeBody는 본문 내용이며, 이 html태그는 chrome에서 F12를 눌러 확인가능**\n",
    "* **class 이름은 바뀔 수 있음을 유의. 예를 들면 세계일보는 articeBody, 조선일보는 articleBodyContents를 사용하기도 함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "article_body = soup2.find('div', id='articeBody')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **텍스트 부분만 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirt_text = article_body.find_all(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **html 태그 없애는 정규표현식이었던듯..?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text = re.sub('[a-zA-Z]', '', str(dirt_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\\ 이런 것들 없애주는 정규표현식**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                              '', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **앞에 달려 있는 \"본문 폰트 설정 안내나눔고딕\"등의 글들을 없애주는 정규표현식**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_text = re.sub('본문 폰트 설정  안내나눔고딕  1맑은고딕  2돋움  3바탕  4폰트 사이즈 1  1폰트 사이즈 2  2폰트 사이즈 3  3폰트 사이즈 4  4폰트 사이즈 5  5',\n",
    "                      '', cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **기사 제목을 가져오기 위해 p 태그에 있는 end_tit class를 가져오기**\n",
    "* **텍스트 다듬는 것은 위와 동일**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"end_tit\">‘아는형님’ 오현경, “강호동이 이상형..고백했으면 사귀었을 것”</p>\n"
     ]
    }
   ],
   "source": [
    "title = soup2.find('p', attrs={'class':'end_tit'})\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‘아는형님’ 오현경, “강호동이 이상형..고백했으면 사귀었을 것”']\n"
     ]
    }
   ],
   "source": [
    "title2 = title.find_all(text=True)\n",
    "print(title2)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‘아는형님’ 오현경, “강호동이 이상형..고백했으면 사귀었을 것”']\n"
     ]
    }
   ],
   "source": [
    "title2 = re.sub('[a-zA-Z]', '', str(title2))\n",
    "print(title2)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘아는형님’ 오현경 “강호동이 이상형고백했으면 사귀었을 것”\n"
     ]
    }
   ],
   "source": [
    "title2 = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]','', title2)\n",
    "print(title2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **기사 정보들 가져오기**\n",
    "* **신문사, 게시 시간 등**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울신문\n"
     ]
    }
   ],
   "source": [
    "press = a.find('span', {'class': 'press'}).text\n",
    "print(press)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3시간전\n"
     ]
    }
   ],
   "source": [
    "date = a.find('span',{'class':'time'}).text\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "article_url.append(article)\n",
    "press_name.append(press)\n",
    "press_time.append(date)  \n",
    "press_title.append(title2)\n",
    "press_text.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Series로 만든 후 DataFrame으로 합친 결과를 내보내면 끝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = Series(press_time)\n",
    "name = Series(press_name)\n",
    "press_url = Series(article_url)\n",
    "title_sr = Series(press_title)\n",
    "text_sr = Series(press_text)\n",
    "\n",
    "url_df = pd.concat([title_sr, name, date, text_sr, press_url], axis=1)\n",
    "url_df.columns = ['TITLE', 'NAME', 'TIME', 'TEXT', 'URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‘아는형님’ 오현경 “강호동이 이상형고백했으면 사귀었을 것”</td>\n",
       "      <td>서울신문</td>\n",
       "      <td>3시간전</td>\n",
       "      <td>서울신문     ‘아는형님’ 오현경 배우 오현경이 “강호동이 이상형이다”고 ...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               TITLE  NAME  TIME  \\\n",
       "0  ‘아는형님’ 오현경 “강호동이 이상형고백했으면 사귀었을 것”  서울신문  3시간전   \n",
       "\n",
       "                                                TEXT  \\\n",
       "0      서울신문     ‘아는형님’ 오현경 배우 오현경이 “강호동이 이상형이다”고 ...   \n",
       "\n",
       "                                                 URL  \n",
       "0  http://news.naver.com/main/read.nhn?mode=LSD&m...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(url_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완성!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, total_pages+1):\n",
    "    url = url_basic + str(i)\n",
    "    \n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, \"lxml\", from_encoding = \"utf-8\")\n",
    "    item = soup.find_all('div', 'info')\n",
    "    \n",
    "    for a in item:\n",
    "        try:\n",
    "            article = a.find('a')['href']\n",
    "        \n",
    "            news_url = urllib.request.urlopen(article)\n",
    "            soup2 = BeautifulSoup(news_url, \"lxml\", from_encoding = \"utf-8\")\n",
    "            article_body = soup2.find('div', id='articleBodyContents')\n",
    "            dirt_text = article_body.find_all(text=True)\n",
    "            cleaned_text = re.sub('[a-zA-Z]', '', str(dirt_text))\n",
    "            cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                              '', cleaned_text)\n",
    "            cleaned_text = re.sub('본문 내용    플레이어     플레이어     오류를 우회하기 위한 함수 추가',\n",
    "                         '', cleaned_text)\n",
    "\n",
    "            title = soup2.find('h3', id='articleTitle')\n",
    "            title2 = title.find_all(text=True)\n",
    "            title2 = re.sub('[a-zA-Z]', '', str(title2))\n",
    "            title2 = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                              '', title2)\n",
    "            \n",
    "        except AttributeError:\n",
    "            try:\n",
    "                article = a.find('a')['href']\n",
    "                news_url = urllib.request.urlopen(article)\n",
    "                soup2 = BeautifulSoup(news_url, \"lxml\", from_encoding = \"utf-8\")\n",
    "                article_body = soup2.find('div', id='articeBody')\n",
    "                dirt_text = article_body.find_all(text=True)\n",
    "                cleaned_text = re.sub('[a-zA-Z]', '', str(dirt_text))\n",
    "                cleaned_text = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                                  '', cleaned_text)\n",
    "                cleaned_text = re.sub('본문 내용    플레이어     플레이어     오류를 우회하기 위한 함수 추가',\n",
    "                             '', cleaned_text)\n",
    "\n",
    "                title = soup2.find('p', attrs={'class':'end_tit'})\n",
    "                title2 = title.find_all(text=True)\n",
    "                title2 = re.sub('[a-zA-Z]', '', str(title2))\n",
    "                title2 = re.sub('[\\{\\}\\[\\]\\/?.,;:|\\)*~`!^\\-_+<>@\\#$%&\\\\\\=\\(\\'\\\"]',\n",
    "                                  '', title2)\n",
    "            \n",
    "            except AttributeError:\n",
    "                title2 = \"Error\"\n",
    "                cleaned_text = \"Error\"\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        press = a.find('span', {'class': 'press'}).text\n",
    "        date = a.find('span',{'class':'time'}).text\n",
    "    \n",
    "        article_url.append(article)\n",
    "        press_name.append(press)\n",
    "        press_time.append(date)\n",
    "        \n",
    "        press_title.append(title2)\n",
    "        press_text.append(cleaned_text)\n",
    "        \n",
    "    random_time = random.randint(20, 40) # 이 부분은 네이버뉴스를 크롤링할 때 페이지 넘기는 속도 제어하는 부분\n",
    "    time.sleep(random_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = Series(press_time)\n",
    "name = Series(press_name)\n",
    "press_url = Series(article_url)\n",
    "title_sr = Series(press_title)\n",
    "text_sr = Series(press_text)\n",
    "\n",
    "url_df = pd.concat([title_sr, name, date, text_sr, press_url], axis=1)\n",
    "url_df.columns = ['TITLE', 'NAME', 'TIME', 'TEXT', 'URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_df.to_csv(r\"D:\\test\" + file_name + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================Finished=================\n"
     ]
    }
   ],
   "source": [
    "print(\"==================Finished=================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
